{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c07155",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy-langdetect\n",
      "  Downloading spacy_langdetect-0.1.2-py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: pytest in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy-langdetect) (7.1.2)\n",
      "Collecting langdetect==1.0.7\n",
      "  Downloading langdetect-1.0.7.zip (998 kB)\n",
      "     -------------------------------------- 998.1/998.1 kB 3.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from langdetect==1.0.7->spacy-langdetect) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from pytest->spacy-langdetect) (22.1.0)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from pytest->spacy-langdetect) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from pytest->spacy-langdetect) (22.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from pytest->spacy-langdetect) (1.0.0)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from pytest->spacy-langdetect) (1.11.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from pytest->spacy-langdetect) (2.0.1)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from pytest->spacy-langdetect) (1.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from pytest->spacy-langdetect) (0.4.6)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.7-py3-none-any.whl size=993439 sha256=3cc6d52711d958e89c4c8aff5fa1ab9534bd210a987bcc73954d29297a8d83f1\n",
      "  Stored in directory: c:\\users\\wei.liu\\appdata\\local\\pip\\cache\\wheels\\87\\8c\\9a\\41c0647bd03b3e11ca6968d3638a4e6e764220adf2886270cb\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect, spacy-langdetect\n",
      "Successfully installed langdetect-1.0.7 spacy-langdetect-0.1.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install spacy-langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "692b32ab",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
      "     -------------------------------------- 587.7/587.7 MB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from en-core-web-lg==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.1.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (22.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wei.liu\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.5.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67147016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from hdbscan import HDBSCAN\n",
    "import hdbscan\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import v_measure_score\n",
    "from umap import UMAP\n",
    "\n",
    "import time\n",
    "from unidecode import unidecode\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38b4bb",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e6f17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27945, 26)\n",
      "(49627, 26)\n"
     ]
    }
   ],
   "source": [
    "# raw data\n",
    "raw_pre = pd.read_excel('data/raw/uncat_oi_pre.xlsx',parse_dates=['Date_Created','Date_Last_Updated', 'DateScheduled'])\n",
    "raw_pst = pd.read_excel('data/raw/uncat_oi_pst.xlsx',parse_dates=['Date_Created','Date_Last_Updated', 'DateScheduled'])\n",
    "\n",
    "print(raw_pre.shape)\n",
    "print(raw_pst.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58fc5782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public API        11735\n",
      "End-User pages     8402\n",
      "CX Console         7333\n",
      "Utilities           429\n",
      "Web Console          41\n",
      "End-User Pages        5\n",
      "Name: Incident_Source, dtype: int64\n",
      "Web Console       18541\n",
      "Agent desktop     12107\n",
      "Public API         7738\n",
      "VBCS Web           4043\n",
      "CX Console         3298\n",
      "End-User Pages     1545\n",
      "Utilities          1144\n",
      "Email               565\n",
      "Live Chat           503\n",
      "Mobile               79\n",
      "None                 63\n",
      "Web                   1\n",
      "Name: Incident_Source, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(raw_pre['Incident_Source'].value_counts())\n",
    "print(raw_pst['Incident_Source'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dedab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data from consoles and utilities only\n",
    "raw_csl_pre = raw_pre[raw_pre['Incident_Source'].isin(['CX Console', 'Utilities', 'Web Console'])]\n",
    "raw_csl_pst = raw_pst[raw_pst['Incident_Source'].isin(['CX Console', 'Utilities', 'Web Console','Agent desktop','Live Chat'])]\n",
    "# data from apps\n",
    "raw_app_pre = raw_pre[raw_pre['Incident_Source'].isin(['End-User pages', 'Oracle Integration', 'Public API'])]\n",
    "raw_app_pst = raw_pst[~raw_pst['Incident_Source'].isin(['CX Console', 'Utilities', 'Web Console','Agent desktop','Live Chat'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd3f902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7803, 26)\n",
      "(35593, 26)\n",
      "(20137, 26)\n",
      "(14034, 26)\n"
     ]
    }
   ],
   "source": [
    "print(raw_csl_pre.shape)\n",
    "print(raw_csl_pst.shape)\n",
    "print(raw_app_pre.shape)\n",
    "print(raw_app_pst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df489550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4751, 26)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_app_pst[raw_app_pst['Year']==2023].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5a9f189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Incident_ID', 'Reference_#', 'Subject', 'Status', 'Incident_Source',\n",
      "       'Category_ID', 'Service', 'Severity', 'District', 'Latitude',\n",
      "       'Longitude', 'Date_Created', 'Date_Last_Updated', 'Department',\n",
      "       'Escalated', 'Year', 'Quarter', 'Week', 'Total_Incidents', 'No_Of_Days',\n",
      "       'Description', 'Language', 'HaulerID', 'DateScheduled', 'DateClosed',\n",
      "       'UserDevice'],\n",
      "      dtype='object')\n",
      "Index(['Incident_ID', 'Reference_#', 'Subject', 'Status', 'Incident_Source',\n",
      "       'Category_ID', 'Service', 'Severity', 'District', 'Latitude',\n",
      "       'Longitude', 'Date_Created', 'Date_Last_Updated', 'Department',\n",
      "       'Escalated', 'Year', 'Quarter', 'Week', 'Total_Incidents', 'No_Of_Days',\n",
      "       'Description', 'Language', 'HaulerID', 'DateScheduled', 'DateClosed',\n",
      "       'UserDevice'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(raw_pre.columns)\n",
    "print(raw_pst.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca4fb2",
   "metadata": {},
   "source": [
    "## Step1. data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ae3c388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_non_letters(name):\n",
    "    '''function that finds out if the documents contains only non english letters '''\n",
    "    import string\n",
    "    char_set = string.ascii_letters\n",
    "    return all((True if x not in char_set else False for x in name))\n",
    "\n",
    "def clear_mctxt(txt, pre_txt, post_text):\n",
    "    '''function that cleans machine translated docs'''\n",
    "    txt = txt[len(pre_txt):txt.index(post_text)]\n",
    "    return txt\n",
    "\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "nlp.add_pipe('language_detector', last=True)\n",
    "\n",
    "\n",
    "# data preprocessing pipeline\n",
    "def pipeline(data, cols, min_len):\n",
    "    '''\n",
    "    data pre-processing pipeline:\n",
    "    1.discard unneeded columns;\n",
    "    2.discard null/empty value;\n",
    "    3.discard descriptions with too little words\n",
    "    4.discard descriptions with only non letters\n",
    "    5.discard descriptions in non english languages\n",
    "    6.replace some special charaters\n",
    "    data: pd dataframe that need to process\n",
    "    cols: list of columns to keep\n",
    "    min_len: descriptions with words less than min_len will be discarded\n",
    "    '''\n",
    "    # keep only cols\n",
    "    df = data[cols]\n",
    "    # keep non null descriptions\n",
    "    df = df[~df['Description'].isnull()]\n",
    "    # make sure description are text\n",
    "    df['Description'] = df['Description'].astype(str)\n",
    "    # discard short descriptions\n",
    "    df['len'] = df['Description'].apply(lambda x: len(str(x).split()))\n",
    "    df = df[(df['len'] >= min_len)]\n",
    "    # discard docs with only non letters\n",
    "    df = df[~df['Description'].apply(only_non_letters)]\n",
    "    # discard test docs\n",
    "    df = df[~df['Description'].str.contains('TEST')]\n",
    "    df = df[~((df['Description'].str.contains('test'))&(df['Description'].str.contains('delete')))]\n",
    "    df = df[~((df['Description'].str.contains('test'))&(df['Description'].str.contains('ignore')))]\n",
    "    # clean machine translated docs\n",
    "    pre_txt = \"(Machine translated text. Report translation errors at https://bit.ly/fll)  \"\n",
    "    post_txt = \"  --Original text--\"\n",
    "    df[df['Description'].str.contains('\\(Machine tran')]['Description']=\\\n",
    "    df[df['Description'].str.contains('\\(Machine tran')]['Description'].apply(lambda x: clear_mctxt(x, pre_txt, post_txt))\n",
    "    \n",
    "    # discard languages other than en \n",
    "    \n",
    "    df['lang'] = [nlp(i)._.language['language'] for i in df['Description']]\n",
    "    df = df[(df['lang']!='es')&(df['lang']!='vi')]\n",
    "    \n",
    "    # process description field to correct characters\n",
    "    for char in [\"â\\?\",\"ã\\?\",\"\\xa0ï¸\\x8f\",'Â\\xa0', 'â\\?¢', \"ï¼\", \"ï»¿ï»¿\"]:\n",
    "        df[\"Description\"] = df[\"Description\"].replace(char, \" \", regex=True)\n",
    "    df[\"Description\"] = df[\"Description\"].replace(\"â\\?\\?\", \"'\", regex=True)\n",
    "    df[\"Description\"] = df[\"Description\"].replace(\"â\\?\\\", \"'\", regex=True)\n",
    "    df[\"Description\"] = df[\"Description\"].replace(\"â€™\", \"'\", regex=True)\n",
    "    df[\"Description\"] = df[\"Description\"].replace(\"Ã\\©\", \"e\", regex=True)\n",
    "    df[\"Description\"] = df[\"Description\"].replace(\"Ã¡\", \"a\", regex=True)\n",
    "    df[\"Description\"] = df[\"Description\"].replace('Ã\\xad', \"i\", regex=True)\n",
    "    df[\"Description\"] = df[\"Description\"].replace(\"Ãº\", \"u\", regex=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30577acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 00:21:42.66\n"
     ]
    }
   ],
   "source": [
    "# clean data\n",
    "cols = ['Incident_ID','Date_Created','Description','Service']\n",
    "st = time.time()\n",
    "df_app_pre = pipeline(raw_app_pre, cols=cols, min_len=2)\n",
    "df_csl_pre = pipeline(raw_csl_pre, cols=cols, min_len=2)\n",
    "df_app_pst = pipeline(raw_app_pst, cols=cols, min_len=2)\n",
    "df_csl_pst = pipeline(raw_csl_pst, cols=cols, min_len=2)\n",
    "\n",
    "elapsed = time.time()-st\n",
    "print(\"Run time: \" + time.strftime(\"%H:%M:%S.{}\".format(str(elapsed % 1)[2:])[:11], time.gmtime(elapsed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b0bfca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Give semi labels to data\n",
    "label_dict = {\"Pothole\":0, \"Abandoned Vehicle\":1, \"Illegal Dumping\":2, \"Other Issues\":-1}\n",
    "for df in [df_app_pre, df_csl_pre, df_app_pst, df_csl_pst]:\n",
    "    df['semi_label'] = df['Service'].apply(lambda x: label_dict[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "071250bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data for modeling\n",
    "df_app_pre.to_csv('data/clean/clean_uncat_app_pre.csv', index=False)\n",
    "df_csl_pre.to_csv('data/clean/clean_uncat_csl_pre.csv', index=False)\n",
    "df_app_pst.to_csv('data/clean/clean_uncat_app_pst.csv', index=False)\n",
    "df_csl_pst.to_csv('data/clean/clean_uncat_csl_pst.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66a627c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data for modeling\n",
    "df_1819.to_csv('data/clean/clean_uncat_pre.csv', index=False)\n",
    "df_2223.to_csv('data/clean/clean_uncat_pst.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091cc176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
